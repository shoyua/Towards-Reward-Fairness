# Towards-Reward-Fairness
This repository contains the code for training the fairness reward model and DPO as described in our paper "Towards Reward Fairness in RLHF: From a Resource Allocation Perspective". 
